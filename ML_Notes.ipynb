{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdga5C5V9kkk0t2rKV7v9a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerry11/ML/blob/main/ML_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCCMd11STrd"
      },
      "source": [
        "# Machine Learning Key Steps\n",
        "\n",
        "1. Define a ML problem and propose a solution\n",
        "2. Construct the dataset\n",
        "  * collect raw data\n",
        "  * indentify features and label sources\n",
        "  * select a sample strategy\n",
        "  * split the data (Or later)\n",
        "3. Transform data\n",
        "  * explore and clean your data\n",
        "  * feature engineering\n",
        "  * Split the data\n",
        "4. Train a model (starting from a simple one)\n",
        "5. Use the model to make predictions\n",
        "\n",
        "[Data Preparation and Feature Engineering](https://developers.google.com/machine-learning/data-prep/?utm_source=mlcc&utm_campaign=mlcc-next-steps&utm_medium=referral&utm_content=data-prep-ss) takes up most of your time when building models, and plays an important role in the success of your models. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQgE6MV2uzev"
      },
      "source": [
        "# Before manipulating the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zH1UFJPy3_H"
      },
      "source": [
        "## 1. Know Your Data \n",
        "Before building models, full unstanding of the data is important, serveral checks are essentail for the success of models:\n",
        "\n",
        "To avoid bias, the fairness of data needs to be evaluated through profiling data:\n",
        "  *   Missing value\n",
        "  *   check the quantile of features\n",
        "  *   Plotting histograms, to see if any Skew exists that mis-represent realitic. (Making sure the training and test sets are similar)\n",
        "\n",
        "The better you know your data, the more insight you'll have as to how to prepare your data for further modelling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4ARiNB-yPyK"
      },
      "source": [
        "###  Facets Tool to view data\n",
        "One useful tool to view and understand data is Facets, consisted from [Facets Overview](https://pair-code.github.io/facets/) and [Facets Dive](https://pair-code.github.io/facets/).\n",
        "\n",
        "Sample codes are listed below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-hCMS_Ux6im"
      },
      "source": [
        "#@title Initial Environment and Data\n",
        "# Run on TensorFlow 2.x\n",
        "%tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "#@title Import revelant modules and install Facets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers\n",
        "# from matplotlib import pyplot as plt\n",
        "# from matplotlib import rcParams\n",
        "# import seaborn as sns\n",
        "\n",
        "# The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "from google.colab import widgets\n",
        "# For facets\n",
        "from IPython.core.display import display, HTML\n",
        "import base64\n",
        "!pip install facets-overview==1.0.0\n",
        "from facets_overview.feature_statistics_generator import FeatureStatisticsGenerator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8beZKdejuxON"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlutjSWZxih1"
      },
      "source": [
        "# Adult Census Income dataset(https://archive.ics.uci.edu/ml/datasets/Census+Income) is used as example\n",
        "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
        "           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
        "           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
        "           \"income_bracket\"]\n",
        "\n",
        "train_csv = tf.keras.utils.get_file('adult.data', \n",
        "  'https://download.mlcc.google.com/mledu-datasets/adult_census_train.csv')\n",
        "test_csv = tf.keras.utils.get_file('adult.data', \n",
        "  'https://download.mlcc.google.com/mledu-datasets/adult_census_test.csv')\n",
        "\n",
        "train_df = pd.read_csv(train_csv, names=COLUMNS, sep=r'\\s*,\\s*', \n",
        "                       engine='python', na_values=\"?\")\n",
        "test_df = pd.read_csv(test_csv, names=COLUMNS, sep=r'\\s*,\\s*', skiprows=[0],\n",
        "                      engine='python', na_values=\"?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0BAkMfsuv3B"
      },
      "source": [
        "#@title Visualize with Facets Overiew\n",
        "fsg = FeatureStatisticsGenerator()\n",
        "dataframes = [\n",
        "    {'table': train_df, 'name': 'trainData'}]\n",
        "censusProto = fsg.ProtoFromDataFrames(dataframes)\n",
        "protostr = base64.b64encode(censusProto.SerializeToString()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
        "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
        "        <facets-overview id=\"elem\"></facets-overview>\n",
        "        <script>\n",
        "          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n",
        "        </script>\"\"\"\n",
        "html = HTML_TEMPLATE.format(protostr=protostr)\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2YI5uffyB6h"
      },
      "source": [
        "#@title Visualize with Facets Dive\n",
        "# Set the Number of Data Points to Visualize in Facets Dive\n",
        "\n",
        "SAMPLE_SIZE = 5000 #@param\n",
        "  \n",
        "train_dive = train_df.sample(SAMPLE_SIZE).to_json(orient='records')\n",
        "\n",
        "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
        "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
        "        <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n",
        "        <script>\n",
        "          var data = {jsonstr};\n",
        "          document.querySelector(\"#elem\").data = data;\n",
        "        </script>\"\"\"\n",
        "html = HTML_TEMPLATE.format(jsonstr=train_dive)\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFpaexOkWxUz"
      },
      "source": [
        "---\n",
        "## Construct Dataset\n",
        "To construct your dataset (and before doing data transformation), you should:\n",
        "1. Collect the raw data.\n",
        "2. Identify feature and label sources.\n",
        "3. Select a sampling strategy.\n",
        "4. Split the data (May do it after transforming the data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc7mJqpiY6Aj"
      },
      "source": [
        "### Collecting the Data\n",
        "#### [The Size of a Dataset](https://developers.google.com/machine-learning/data-prep/construct/collect/data-size-quality#the-size-of-a-data-set)\n",
        "\"As a rough rule of thumb, your model should train on **at least an order of magnitude more examples than trainable parameters**. Simple models on large data sets generally beat fancy models on small data sets.\"\n",
        "\n",
        "#### [The Quality of a Dataset](https://developers.google.com/machine-learning/data-prep/construct/collect/data-size-quality#the-quality-of-a-data-set)\n",
        "\"A quality data set is one that lets you succeed with the business problem you care about. In other words, the data is good if it accomplishes its intended task.\"\n",
        "\n",
        "Certain aspects of quality are more likely to build better-performing models:\n",
        "1. [Reliability](https://developers.google.com/machine-learning/data-prep/construct/collect/data-size-quality#reliability) refers to the degree to which you can trust your data. \n",
        "2. [Feature Representation](https://developers.google.com/machine-learning/data-prep/construct/collect/data-size-quality#feature-representation): \"Always consider what data is available to your model at prediction time. During training, use only the features that you'll have available in serving.\"\n",
        "3. [Minimize Skew](https://developers.google.com/machine-learning/data-prep/construct/collect/data-size-quality#training-versus-prediction):\n",
        "\"Make sure your training set is representative of your serving traffic.\"\n",
        "\n",
        "### [Joining Logs](https://developers.google.com/machine-learning/data-prep/construct/collect/joining-logs)\n",
        "### [label Sources](https://developers.google.com/machine-learning/data-prep/construct/collect/label-sources)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMYnxj0-gMMj"
      },
      "source": [
        "### [Sampling and Splitting Data](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/sampling)\n",
        "\n",
        "1. Sampling - Imbalanced Data and how to downsample and upweight\n",
        "2. Splitting - Randon Splitting isn't always the Best.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PIoUoCwnT0r"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## [Transform Data](https://developers.google.com/machine-learning/data-prep/transform/introduction)\n",
        "\n",
        "### Numeric Data \n",
        "\n",
        "\n",
        "*   [Normalization](https://developers.google.com/machine-learning/data-prep/transform/normalization)\n",
        "  * Scaling to a range \n",
        "      * Upper and Lower bounds are known, few/no outliers\n",
        "      * Data is approximately uniformly distributed\n",
        "  * feature clipping\n",
        "      * To deal with extreme outliers, cap feature values with a min/max values.\n",
        "      One simple strategy is to clip by z-score to +/- 3 \n",
        "  * log scaling\n",
        "      * suitable for data wiwth *power law distribution*. \n",
        "  * z-score\n",
        "*   [Bucketing](https://developers.google.com/machine-learning/data-prep/transform/bucketing)\n",
        "Sometimes when there is no clear correlation between the label and one numeric feature, it is worth considering transforming it into categorical feature, using a set of thresholds. Techniques for this purpose is called bucketing. \n",
        "\n",
        "To bucketize the numeric features, there are two types of approaches:\n",
        "1. Buckets with **equally spaced** boundaries:\n",
        "  the boundaries are fixed and encompass the same range\n",
        "2. Buckets with **quantile** boundaries: \n",
        "  each bucket has the same number of records.  \n",
        "\n",
        "\n",
        "\n",
        "### [Categorical Data](https://developers.google.com/machine-learning/data-prep/transform/transform-categorical)\n",
        "1. Vocabulary\n",
        "2. Hashing\n",
        "3. Hybrid of Hashing and Vocabulary\n",
        "\n",
        "\n",
        "NOTE that embeddings are not a typical data transformation — they are part of the model, and functionally are equivalent to a layer of weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFz_8R13RKbl"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Some Effective Quidelines for ML\n",
        "* Keep your first model simple.\n",
        "* Focus on ensuring data pipeline correctness.\n",
        "* Use a simple, observable metric for training & evaluation.\n",
        "* Own and monitor your input features.\n",
        "* Treat your model configuration as code: review it, check it in.\n",
        "* Write down the results of all experiments, especially \"failures.\"\n",
        "\n",
        "Source from Google's [Machine learning crash course](https://developers.google.com/machine-learning/crash-course)"
      ]
    }
  ]
}